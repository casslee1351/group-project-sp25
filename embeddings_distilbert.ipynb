{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\pokem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('song_lyrics.csv', nrows=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>tag</th>\n",
       "      <th>artist</th>\n",
       "      <th>year</th>\n",
       "      <th>views</th>\n",
       "      <th>features</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>id</th>\n",
       "      <th>language_cld3</th>\n",
       "      <th>language_ft</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Killa Cam</td>\n",
       "      <td>rap</td>\n",
       "      <td>Cam'ron</td>\n",
       "      <td>2004</td>\n",
       "      <td>173166</td>\n",
       "      <td>{\"Cam\\\\'ron\",\"Opera Steve\"}</td>\n",
       "      <td>[Chorus: Opera Steve &amp; Cam'ron]\\nKilla Cam, Ki...</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can I Live</td>\n",
       "      <td>rap</td>\n",
       "      <td>JAY-Z</td>\n",
       "      <td>1996</td>\n",
       "      <td>468624</td>\n",
       "      <td>{}</td>\n",
       "      <td>[Produced by Irv Gotti]\\n\\n[Intro]\\nYeah, hah,...</td>\n",
       "      <td>3</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Forgive Me Father</td>\n",
       "      <td>rap</td>\n",
       "      <td>Fabolous</td>\n",
       "      <td>2003</td>\n",
       "      <td>4743</td>\n",
       "      <td>{}</td>\n",
       "      <td>Maybe cause I'm eatin\\nAnd these bastards fien...</td>\n",
       "      <td>4</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Down and Out</td>\n",
       "      <td>rap</td>\n",
       "      <td>Cam'ron</td>\n",
       "      <td>2004</td>\n",
       "      <td>144404</td>\n",
       "      <td>{\"Cam\\\\'ron\",\"Kanye West\",\"Syleena Johnson\"}</td>\n",
       "      <td>[Produced by Kanye West and Brian Miller]\\n\\n[...</td>\n",
       "      <td>5</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fly In</td>\n",
       "      <td>rap</td>\n",
       "      <td>Lil Wayne</td>\n",
       "      <td>2005</td>\n",
       "      <td>78271</td>\n",
       "      <td>{}</td>\n",
       "      <td>[Intro]\\nSo they ask me\\n\"Young boy\\nWhat you ...</td>\n",
       "      <td>6</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               title  tag     artist  year   views  \\\n",
       "0          Killa Cam  rap    Cam'ron  2004  173166   \n",
       "1         Can I Live  rap      JAY-Z  1996  468624   \n",
       "2  Forgive Me Father  rap   Fabolous  2003    4743   \n",
       "3       Down and Out  rap    Cam'ron  2004  144404   \n",
       "4             Fly In  rap  Lil Wayne  2005   78271   \n",
       "\n",
       "                                       features  \\\n",
       "0                   {\"Cam\\\\'ron\",\"Opera Steve\"}   \n",
       "1                                            {}   \n",
       "2                                            {}   \n",
       "3  {\"Cam\\\\'ron\",\"Kanye West\",\"Syleena Johnson\"}   \n",
       "4                                            {}   \n",
       "\n",
       "                                              lyrics  id language_cld3  \\\n",
       "0  [Chorus: Opera Steve & Cam'ron]\\nKilla Cam, Ki...   1            en   \n",
       "1  [Produced by Irv Gotti]\\n\\n[Intro]\\nYeah, hah,...   3            en   \n",
       "2  Maybe cause I'm eatin\\nAnd these bastards fien...   4            en   \n",
       "3  [Produced by Kanye West and Brian Miller]\\n\\n[...   5            en   \n",
       "4  [Intro]\\nSo they ask me\\n\"Young boy\\nWhat you ...   6            en   \n",
       "\n",
       "  language_ft language  \n",
       "0          en       en  \n",
       "1          en       en  \n",
       "2          en       en  \n",
       "3          en       en  \n",
       "4          en       en  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### clean up current lyrics\n",
    "### stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [[, Chorus, :, Opera, Steve, &, Cam'ron, ], Ki...\n",
       "1       [[, Produced, by, Irv, Gotti, ], [, Intro, ], ...\n",
       "2       [Maybe, cause, I, 'm, eatin, And, these, basta...\n",
       "3       [[, Produced, by, Kanye, West, and, Brian, Mil...\n",
       "4       [[, Intro, ], So, they, ask, me, '', Young, bo...\n",
       "                              ...                        \n",
       "9995    [[, Hook, ], Wish, I, would, a, died, for, you...\n",
       "9996    [Yeah, Studio, rap, productions, (, this, is, ...\n",
       "9997    [[, Verse, 1, ], Hey, it, 's, the, Martin, and...\n",
       "9998    [Yeah, Some, people, wonder, ,, ya, know, ?, T...\n",
       "9999    [[, Intro, :, Bizzy, Bone, ], Rest, in, peace,...\n",
       "Name: tokenized_text, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokenized_text'] = df[\"lyrics\"].apply(word_tokenize)\n",
    "df['tokenized_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # clean your sentences\n",
    "# stopwords = [YOUR_STOPWORDS_HERE]\n",
    "# cleaned_sentences = []\n",
    "# for sentence in sentences:\n",
    "#   cleaned = [word.lower() for word in sentence]\n",
    "#   cleaned = [word for word in cleaned if word not in stopwords]\n",
    "#   cleaned_sentences.append(cleaned)\n",
    "\n",
    "# build a word2vec model on your dataset\n",
    "sentences = df['tokenized_text'].tolist()\n",
    "base_model = Word2Vec(vector_size=300, min_count=5)\n",
    "base_model.build_vocab(sentences)\n",
    "total_examples = base_model.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_word2vec(sentences):\n",
    "  \"\"\"\n",
    "  apply_word2vec\n",
    "  params: senetences -> 'tokenized_text'\n",
    "  returns: word2vec model\n",
    "  \"\"\"\n",
    "  base_model = Word2Vec(vector_size=300, min_count=5)\n",
    "  base_model.build_vocab(sentences)\n",
    "  return base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = apply_word2vec(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vector representation of the word 'sentence'\n",
    "vector = model.wv[\"sentence\"]\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding: GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add GloVe's vocabulary & weights\n",
    "base_model.build_vocab([list(glove_vectors.vocab.keys())], update=True)\n",
    "\n",
    "# train on your data\n",
    "base_model.train(sentences, total_examples=total_examples, epochs=base_model.epochs)\n",
    "base_model_wv = base_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 104.8/104.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "model_glove_twitter = api.load(\"glove-twitter-25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('clegg', 0.9653650522232056),\n",
       " ('miliband', 0.9515050053596497),\n",
       " ('bachmann', 0.9484400749206543),\n",
       " ('mcconnell', 0.9416398406028748),\n",
       " ('carney', 0.9340257048606873),\n",
       " ('coulter', 0.9311323165893555),\n",
       " ('boehner', 0.9286302328109741),\n",
       " ('santorum', 0.9269059300422668),\n",
       " ('farage', 0.9193653464317322),\n",
       " ('mourdock', 0.9186689853668213)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_glove_twitter.most_similar(\"pelosi\",topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding: DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import library\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "#load DistilBERT tokenizer and a pretrained model to avoid training from scratch\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize and obtain embeddings\n",
    "def get_lyrics_embedding(lyrics):\n",
    "    tokens = tokenizer(\n",
    "        lyrics,\n",
    "        truncation = True,\n",
    "        padding = True, \n",
    "        max_length = 512, #DistilBERT has a max token limit of 512\n",
    "        return_tensors = \"pt\" #converting output as PyTorch since DistilBERT expects tensors not token IDs\n",
    "        )\n",
    "    \n",
    "    with torch.no_grad(): #removes gradient calculation to save memory usage since inferences are not needed as we are predicting, not training\n",
    "        output = model(**tokens)\n",
    "\n",
    "    cls_embedding = output.last_hidden_state[:, 0] #extract first token with CLS\n",
    "    cls_embedding = cls_embedding.detach() #detach from PyTorch's gradient computation \n",
    "    cls_embedding = cls_embedding.cpu() #converting tensor to CPU to ensure compatability (ie. NumPy array conversion)\n",
    "    cls_embedding = cls_embedding.squeeze() #remove any extra dimensions\n",
    "    \n",
    "    embedding = cls_embedding.numpy() #converting into NumPy array\n",
    "    return embedding\n",
    "\n",
    "#converting lyrics into embeddings using nrows\n",
    "embeddings = np.array([get_lyrics_embedding(lyric) for lyric in df['lyrics']])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "song_lyrics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
