{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nccru\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\nccru\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "import gensim.downloader as api\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('song_lyrics.csv', nrows=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### clean up current lyrics\n",
    "### remove chorus:, intro:, etc.\n",
    "### stop words, punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_between_brackets(text):\n",
    "  \"\"\"Removes all text between any matching pair of brackets, including the brackets themselves.\"\"\"\n",
    "  return re.sub(r'\\[.*?\\]', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanse_lyrics(df):\n",
    "    sw = stopwords.words('english')\n",
    "\n",
    "    df['cleaned_lyrics'] = df['lyrics'].apply(remove_between_brackets)\n",
    "    df['cleaned_lyrics'] = df['cleaned_lyrics'].str.lower()\n",
    "    df['cleaned_lyrics'] = df['cleaned_lyrics'].apply(remove_stopwords)\n",
    "    df['tokenized_text'] = df[\"cleaned_lyrics\"].apply(word_tokenize)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>tag</th>\n",
       "      <th>artist</th>\n",
       "      <th>year</th>\n",
       "      <th>views</th>\n",
       "      <th>features</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>id</th>\n",
       "      <th>language_cld3</th>\n",
       "      <th>language_ft</th>\n",
       "      <th>language</th>\n",
       "      <th>cleaned_lyrics</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Killa Cam</td>\n",
       "      <td>rap</td>\n",
       "      <td>Cam'ron</td>\n",
       "      <td>2004</td>\n",
       "      <td>173166</td>\n",
       "      <td>{\"Cam\\\\'ron\",\"Opera Steve\"}</td>\n",
       "      <td>[Chorus: Opera Steve &amp; Cam'ron]\\nKilla Cam, Ki...</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>killa cam, killa cam, cam killa cam, killa cam...</td>\n",
       "      <td>[killa, cam, ,, killa, cam, ,, cam, killa, cam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can I Live</td>\n",
       "      <td>rap</td>\n",
       "      <td>JAY-Z</td>\n",
       "      <td>1996</td>\n",
       "      <td>468624</td>\n",
       "      <td>{}</td>\n",
       "      <td>[Produced by Irv Gotti]\\n\\n[Intro]\\nYeah, hah,...</td>\n",
       "      <td>3</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>yeah, hah, yeah, roc-a-fella invite somethin' ...</td>\n",
       "      <td>[yeah, ,, hah, ,, yeah, ,, roc-a-fella, invite...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Forgive Me Father</td>\n",
       "      <td>rap</td>\n",
       "      <td>Fabolous</td>\n",
       "      <td>2003</td>\n",
       "      <td>4743</td>\n",
       "      <td>{}</td>\n",
       "      <td>Maybe cause I'm eatin\\nAnd these bastards fien...</td>\n",
       "      <td>4</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>maybe cause i'm eatin bastards fiend grub carr...</td>\n",
       "      <td>[maybe, cause, i, 'm, eatin, bastards, fiend, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Down and Out</td>\n",
       "      <td>rap</td>\n",
       "      <td>Cam'ron</td>\n",
       "      <td>2004</td>\n",
       "      <td>144404</td>\n",
       "      <td>{\"Cam\\\\'ron\",\"Kanye West\",\"Syleena Johnson\"}</td>\n",
       "      <td>[Produced by Kanye West and Brian Miller]\\n\\n[...</td>\n",
       "      <td>5</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>ugh, killa! baby! kanye, 1970s heron flow, huh...</td>\n",
       "      <td>[ugh, ,, killa, !, baby, !, kanye, ,, 1970s, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fly In</td>\n",
       "      <td>rap</td>\n",
       "      <td>Lil Wayne</td>\n",
       "      <td>2005</td>\n",
       "      <td>78271</td>\n",
       "      <td>{}</td>\n",
       "      <td>[Intro]\\nSo they ask me\\n\"Young boy\\nWhat you ...</td>\n",
       "      <td>6</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>ask \"young boy gon' second time around? gon' c...</td>\n",
       "      <td>[ask, ``, young, boy, gon, ', second, time, ar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               title  tag     artist  year   views  \\\n",
       "0          Killa Cam  rap    Cam'ron  2004  173166   \n",
       "1         Can I Live  rap      JAY-Z  1996  468624   \n",
       "2  Forgive Me Father  rap   Fabolous  2003    4743   \n",
       "3       Down and Out  rap    Cam'ron  2004  144404   \n",
       "4             Fly In  rap  Lil Wayne  2005   78271   \n",
       "\n",
       "                                       features  \\\n",
       "0                   {\"Cam\\\\'ron\",\"Opera Steve\"}   \n",
       "1                                            {}   \n",
       "2                                            {}   \n",
       "3  {\"Cam\\\\'ron\",\"Kanye West\",\"Syleena Johnson\"}   \n",
       "4                                            {}   \n",
       "\n",
       "                                              lyrics  id language_cld3  \\\n",
       "0  [Chorus: Opera Steve & Cam'ron]\\nKilla Cam, Ki...   1            en   \n",
       "1  [Produced by Irv Gotti]\\n\\n[Intro]\\nYeah, hah,...   3            en   \n",
       "2  Maybe cause I'm eatin\\nAnd these bastards fien...   4            en   \n",
       "3  [Produced by Kanye West and Brian Miller]\\n\\n[...   5            en   \n",
       "4  [Intro]\\nSo they ask me\\n\"Young boy\\nWhat you ...   6            en   \n",
       "\n",
       "  language_ft language                                     cleaned_lyrics  \\\n",
       "0          en       en  killa cam, killa cam, cam killa cam, killa cam...   \n",
       "1          en       en  yeah, hah, yeah, roc-a-fella invite somethin' ...   \n",
       "2          en       en  maybe cause i'm eatin bastards fiend grub carr...   \n",
       "3          en       en  ugh, killa! baby! kanye, 1970s heron flow, huh...   \n",
       "4          en       en  ask \"young boy gon' second time around? gon' c...   \n",
       "\n",
       "                                      tokenized_text  \n",
       "0  [killa, cam, ,, killa, cam, ,, cam, killa, cam...  \n",
       "1  [yeah, ,, hah, ,, yeah, ,, roc-a-fella, invite...  \n",
       "2  [maybe, cause, i, 'm, eatin, bastards, fiend, ...  \n",
       "3  [ugh, ,, killa, !, baby, !, kanye, ,, 1970s, h...  \n",
       "4  [ask, ``, young, boy, gon, ', second, time, ar...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = cleanse_lyrics(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # clean your sentences\n",
    "# stopwords = [YOUR_STOPWORDS_HERE]\n",
    "# cleaned_sentences = []\n",
    "# for sentence in sentences:\n",
    "#   cleaned = [word.lower() for word in sentence]\n",
    "#   cleaned = [word for word in cleaned if word not in stopwords]\n",
    "#   cleaned_sentences.append(cleaned)\n",
    "\n",
    "# build a word2vec model on your dataset\n",
    "sentences = df['tokenized_text'].tolist()\n",
    "# base_model = Word2Vec(vector_size=100, min_count=5)\n",
    "# base_model.build_vocab(sentences)\n",
    "# total_examples = base_model.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model.train(sentences, total_examples=total_examples, epochs=base_model.epochs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(w for w in base_model.wv.index_to_key)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model.wv.vectors[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_word2vec(sentences):\n",
    "  \"\"\"\n",
    "  apply_word2vec\n",
    "  params: sentences -> 'tokenized_text'\n",
    "  returns: word2vec model\n",
    "  \n",
    "  Access vectors from base_model.wv.vectors and base_model.wv.index_to_key\n",
    "  \"\"\"\n",
    "  base_model = Word2Vec(vector_size=100, min_count=5)\n",
    "  base_model.build_vocab(sentences)\n",
    "  # base_model.train(sentences, total_examples=base_model.corpus_count, epochs=base_model.epochs) \n",
    "  return base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = apply_word2vec(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding: GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_glove(sentences, model=\"glove-wiki-gigaword-100\"):\n",
    "\n",
    "    print(\"Models available for use:\")\n",
    "    print(list(gensim.downloader.info()['models'].keys()))\n",
    "\n",
    "    glove_model = api.load(model)\n",
    "\n",
    "    ### initialize model\n",
    "    base_model = Word2Vec(vector_size=100, min_count=1)\n",
    "    base_model.build_vocab(sentences)\n",
    "    total_examples = base_model.corpus_count\n",
    "\n",
    "    base_model.build_vocab(glove_model.index_to_key, update=True)\n",
    "    base_model.train(sentences, total_examples=total_examples, epochs=base_model.epochs)\n",
    "\n",
    "    return base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "glove_model = api.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models available for use:\n",
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x282a5cf6250>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_glove(sentences = sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### apply_glove and embedding matrix are not related to each. They are separate implementations depending on needed format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_glove_matrix(df):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(df['cleaned_lyrics'])\n",
    "\n",
    "    max_length = max(len(data) for data in df['cleaned_lyrics'])\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(word_index)    \n",
    "\n",
    "    # padding text data\n",
    "    sequences = tokenizer.texts_to_sequences(df['cleaned_lyrics'])\n",
    "    padded_seq = pad_sequences(sequences, maxlen=12630, padding='post', truncating='post')\n",
    "\n",
    "    # create embedding index\n",
    "    embedding_index = {}\n",
    "    with open('glove.42B.300d.txt', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embedding_index[word] = coefs\n",
    "\n",
    "    # create embedding matrix\n",
    "    embedding_matrix = np.zeros((vocab_size+1, 300))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding: BERT and DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nccru\\anaconda3\\envs\\cse-6242-project\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\nccru\\anaconda3\\envs\\cse-6242-project\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\nccru\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "#import library\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch\n",
    "# import numpy as np\n",
    "\n",
    "#load DistilBERT tokenizer and a pretrained model to avoid training from scratch\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize and obtain embeddings\n",
    "def get_lyrics_embedding(lyrics):\n",
    "    tokens = tokenizer(\n",
    "        lyrics,\n",
    "        truncation = True,\n",
    "        padding = True, \n",
    "        max_length = 512, #DistilBERT has a max token limit of 512\n",
    "        return_tensors = \"pt\" #converting output as PyTorch since DistilBERT expects tensors not token IDs\n",
    "        )\n",
    "    \n",
    "    with torch.no_grad(): #removes gradient calculation to save memory usage since inferences are not needed as we are predicting, not training\n",
    "        output = model(**tokens)\n",
    "\n",
    "    cls_embedding = output.last_hidden_state[:, 0] #extract first token with CLS\n",
    "    cls_embedding = cls_embedding.detach() #detach from PyTorch's gradient computation \n",
    "    cls_embedding = cls_embedding.cpu() #converting tensor to CPU to ensure compatability (ie. NumPy array conversion)\n",
    "    cls_embedding = cls_embedding.squeeze() #remove any extra dimensions\n",
    "    \n",
    "    embedding = cls_embedding.numpy() #converting into NumPy array\n",
    "    return embedding\n",
    "\n",
    "#converting lyrics into embeddings using nrows\n",
    "embeddings = np.array([get_lyrics_embedding(lyric) for lyric in df['lyrics']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Are we using Keras / PyTorch?\n",
    "This may change the format and implementation of the current method of embedding."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse-6242-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
