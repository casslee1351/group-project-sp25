{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE-6242 - Team 157 - Group Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TODO:__  \n",
    "1. Remove other stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global assumption panel\n",
    "\n",
    "## general\n",
    "TRAIN_MODELS = True # if true, we add a lot of time\n",
    "\n",
    "## data gathering\n",
    "READ_CLEANED_OR_RAW = 'Clean' # ['Clean', 'Raw']\n",
    "N_DATA_ROWS_PER_GENRE = 50 # ['All', int]\n",
    "\n",
    "## embedding\n",
    "EMBED_STRATEGY = 'GloVe' # ['DistilBERT', 'GloVe']\n",
    "MAX_GLOVE_LENGTH = 200 # [None, int]\n",
    "\n",
    "## modeling - preprocessing\n",
    "VAL_PCT = 0.15 # the percent of data we want to withhold for testing\n",
    "BATCH_SIZE = 32 # bigger means faster training, but more memory use\n",
    "\n",
    "## modeling - architecture\n",
    "HIDDEN_SIZE = 256\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.2\n",
    "NUM_HEADS = 8 if EMBED_STRATEGY == 'DistilBERT' else 6\n",
    "if EMBED_STRATEGY == 'GloVe':\n",
    "    assert 300 % NUM_HEADS == 0\n",
    "elif EMBED_STRATEGY == 'DistilBERT':\n",
    "    assert 768 % NUM_HEADS == 0\n",
    "\n",
    "## modeling - training\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 50\n",
    "PATIENCE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nccru\\anaconda3\\envs\\cse6242-team157-project\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\nccru\\anaconda3\\envs\\cse6242-team157-project\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# packages\n",
    "\n",
    "## general use\n",
    "from datetime import datetime\n",
    "\n",
    "## torch\n",
    "import torch\n",
    "\n",
    "## project code\n",
    "from project_code import data_gathering, genre_classification\n",
    "from embedding import distilbert, glove\n",
    "from modeling import preprocessing, training\n",
    "from architectures import nn_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script Start Time = 2025-03-30 13:46:25\n",
      "Device = cpu\n"
     ]
    }
   ],
   "source": [
    "# more global assumptions\n",
    "START_TIME = datetime.now()\n",
    "print(f'Script Start Time = {START_TIME.strftime(\"%Y-%m-%d %H:%M:%S\")}')\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device = {DEVICE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genre Counts Before Resampling:\n",
      "\tpop: 394195\n",
      "\trap: 394195\n",
      "\trock: 394195\n",
      "\trb: 155082\n",
      "\tmisc: 140986\n",
      "\tcountry: 86658\n",
      "\n",
      "Genre Counts After Resampling:\n",
      "\tcountry: 50\n",
      "\tmisc: 50\n",
      "\tpop: 50\n",
      "\trap: 50\n",
      "\trb: 50\n",
      "\trock: 50\n",
      "\n",
      "Cleaned Lyrics: Shape = (300, 2)\n",
      "\tColumns = ['lyrics', 'genre']\n",
      "Genre Mapping = {0: 'country', 1: 'misc', 2: 'pop', 3: 'rap', 4: 'rb', 5: 'rock'}\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "if TRAIN_MODELS:\n",
    "    if READ_CLEANED_OR_RAW == 'Raw': \n",
    "        lyrics = data_gathering.read_and_clean_raw_lyrics(\n",
    "            n_rows = 'All',\n",
    "            exclude_non_english = True,\n",
    "            resample_genres = True,\n",
    "            save_data = True\n",
    "        )\n",
    "    elif READ_CLEANED_OR_RAW == 'Clean':\n",
    "        lyrics, genre_map = data_gathering.read_cleaned_lyrics(\n",
    "            n_rows_per_genre = N_DATA_ROWS_PER_GENRE\n",
    "        )\n",
    "else:\n",
    "    genre_map = {0: 'country', 1: 'misc', 2: 'pop', 3: 'rap', 4: 'rb', 5: 'rock'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed + Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting GloVe Embedding Index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1917494it [01:51, 17182.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Converting Word Indices to GloVe Vectors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:00<00:00, 3758.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GloVe Embedded Lyrics: Shape = (n_songs, max_seq_len, embed_len) = torch.Size([300, 200, 300])\n",
      "\tPadded Sequences: Shape = (n_songs, max_seq_len) = (300, 200)\n"
     ]
    }
   ],
   "source": [
    "# generate embeddings using word2vec\n",
    "if TRAIN_MODELS:\n",
    "    if EMBED_STRATEGY == 'GloVe':\n",
    "        lyrics_embed, glove_index = glove.embed_all_lyrics(\n",
    "            data = lyrics,\n",
    "            target_col = 'lyrics',\n",
    "            custom_max_seq_len = MAX_GLOVE_LENGTH\n",
    "        )\n",
    "    elif EMBED_STRATEGY == 'DistilBERT':\n",
    "        # lyrics_embed = distilbert.distilbert_embed_all_docs(data = lyrics, target_col = 'lyrics')\n",
    "        lyrics_embed = distilbert.embed_all_lyrics_v2(\n",
    "            data = lyrics,\n",
    "            target_col = 'lyrics',\n",
    "            batch_size = BATCH_SIZE * 2\n",
    "        )\n",
    "else:\n",
    "    if EMBED_STRATEGY == \"GloVe\":\n",
    "        glove_index = glove.read_glove_embedding_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 7 Batches of Size 32 For Training\n",
      "Val: 2 Batches of Size 32 For Training\n",
      "Test: 2 Batches of Size 32 For Final Eval\n"
     ]
    }
   ],
   "source": [
    "# create data loaders (train, val) and data sets (test)\n",
    "if TRAIN_MODELS:\n",
    "    lyrics_train, lyrics_val, lyrics_test = preprocessing.create_datasets(\n",
    "        data_embed = lyrics_embed,\n",
    "        labels = lyrics['genre'],\n",
    "        label_mapping = genre_map,\n",
    "        val_pct = VAL_PCT,\n",
    "        batch_size = BATCH_SIZE\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train - RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseRNN(\n",
       "  (rnn): GRU(300, 256, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (fc): Linear(in_features=512, out_features=6, bias=True)\n",
       "  (softmax): Softmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the baseline RNN Model\n",
    "if TRAIN_MODELS:\n",
    "    if EMBED_STRATEGY == 'GloVe':\n",
    "        n_songs, max_seq_len, embed_dim = lyrics_embed.shape \n",
    "    elif EMBED_STRATEGY == 'DistilBERT':\n",
    "        n_songs, embed_dim = lyrics_embed.shape\n",
    "else:\n",
    "    if EMBED_STRATEGY == 'GloVe':\n",
    "        embed_dim = 300 \n",
    "    elif EMBED_STRATEGY == 'DistilBert':\n",
    "        embed_dim = 768\n",
    "\n",
    "base_model = nn_clf.BaseRNN(\n",
    "    input_dim = embed_dim,\n",
    "    hidden_dim = HIDDEN_SIZE,\n",
    "    output_dim = len(genre_map),\n",
    "    type = 'GRU',\n",
    "    num_layers = NUM_LAYERS,\n",
    "    dropout = DROPOUT\n",
    ").to(DEVICE)\n",
    "base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy = 17.78%\n"
     ]
    }
   ],
   "source": [
    "# evaluate model performance - pre training\n",
    "if TRAIN_MODELS:\n",
    "    pre_train_acc = training.evaluate_nn_model_against_test_set(\n",
    "        model = base_model,\n",
    "        test_dataset = lyrics_test\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:23<00:00,  3.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 / 50] Train Loss = 1.7737, Val Loss = 1.6382 **New Best Model**\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:12<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 / 50] Train Loss = 1.5255, Val Loss = 1.5224 **New Best Model**\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:05<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 / 50] Train Loss = 1.3447, Val Loss = 1.4277 **New Best Model**\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:06<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 / 50] Train Loss = 1.1169, Val Loss = 1.5077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:05<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 / 50] Train Loss = 0.7902, Val Loss = 1.5509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:05<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 / 50] Train Loss = 0.5466, Val Loss = 1.8882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:05<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 / 50] Train Loss = 0.4289, Val Loss = 2.3384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:06<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8 / 50] Train Loss = 0.2944, Val Loss = 2.3542\n",
      "Early Stopping Triggered. Training Stopped.\n",
      "\tBest Epoch = 2, Best Val Loss = 1.4277328848838806\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "if TRAIN_MODELS:\n",
    "    training.nn_training(\n",
    "        model = base_model,\n",
    "        train_loader = lyrics_train,\n",
    "        val_loader = lyrics_val,\n",
    "        embed_strategy = EMBED_STRATEGY,\n",
    "        learning_rate = LEARNING_RATE,\n",
    "        num_epochs = NUM_EPOCHS,\n",
    "        patience = PATIENCE,\n",
    "        verbose = True,\n",
    "        print_every = 1\n",
    "    )\n",
    "else:\n",
    "    state_dict = torch.load(f'models/{EMBED_STRATEGY}_BaseRNN_Trained.pth', map_location = torch.device(DEVICE))\n",
    "    base_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy = 22.22%\n",
      "Training Improvement On Accuracy = +4.44%\n"
     ]
    }
   ],
   "source": [
    "# evaluate model performance - post traing\n",
    "if TRAIN_MODELS:\n",
    "    post_train_acc = training.evaluate_nn_model_against_test_set(\n",
    "        model = base_model,\n",
    "        test_dataset = lyrics_test\n",
    "    )\n",
    "\n",
    "    print(f'Training Improvement On Accuracy = +{(post_train_acc - pre_train_acc) * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train - Homemade Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepTransformer(\n",
       "  (attention_layers): ModuleList(\n",
       "    (0-1): 2 x MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=300, out_features=300, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (ffn_layers): ModuleList(\n",
       "    (0-1): 2 x Sequential(\n",
       "      (0): Linear(in_features=300, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=256, out_features=300, bias=True)\n",
       "      (3): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (norm_layers_attn): ModuleList(\n",
       "    (0-1): 2 x LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (norm_layers_ffn): ModuleList(\n",
       "    (0-1): 2 x LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (fc): Linear(in_features=300, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the transformer model\n",
    "if TRAIN_MODELS:\n",
    "    if EMBED_STRATEGY == 'GloVe':\n",
    "        n_songs, max_seq_len, embed_dim = lyrics_embed.shape \n",
    "    elif EMBED_STRATEGY == 'DistilBERT':\n",
    "        n_songs, embed_dim = lyrics_embed.shape\n",
    "else:\n",
    "    if EMBED_STRATEGY == 'Glove':\n",
    "        embed_dim = 300 \n",
    "    elif EMBED_STRATEGY == 'DistilBERT':\n",
    "        embed_dim = 768\n",
    "    \n",
    "transformer_model = nn_clf.DeepTransformer(\n",
    "    input_dim = embed_dim,\n",
    "    num_heads = NUM_HEADS,\n",
    "    hidden_dim = HIDDEN_SIZE,\n",
    "    output_dim = len(genre_map),\n",
    "    num_layers = NUM_LAYERS,\n",
    "    dropout = DROPOUT\n",
    ").to(DEVICE)\n",
    "transformer_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy = 13.33%\n"
     ]
    }
   ],
   "source": [
    "# evaluate model performance - pre training\n",
    "if TRAIN_MODELS:\n",
    "    pre_train_acc = training.evaluate_nn_model_against_test_set(\n",
    "        model = transformer_model,\n",
    "        test_dataset = lyrics_test\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:03<00:00,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 / 50] Train Loss = 2.2021, Val Loss = 1.9200 **New Best Model**\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:02<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 / 50] Train Loss = 1.7065, Val Loss = 1.6690 **New Best Model**\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:02<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 / 50] Train Loss = 1.4169, Val Loss = 1.4308 **New Best Model**\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:02<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 / 50] Train Loss = 1.1635, Val Loss = 1.6385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:02<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 / 50] Train Loss = 0.8595, Val Loss = 1.3617 **New Best Model**\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:02<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 / 50] Train Loss = 0.6144, Val Loss = 1.4164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:02<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 / 50] Train Loss = 0.3350, Val Loss = 1.8297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:02<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8 / 50] Train Loss = 0.1675, Val Loss = 1.9894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:03<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 / 50] Train Loss = 0.0919, Val Loss = 1.8917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:03<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 / 50] Train Loss = 0.0538, Val Loss = 2.2654\n",
      "Early Stopping Triggered. Training Stopped.\n",
      "\tBest Epoch = 4, Best Val Loss = 1.3617021441459656\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "if TRAIN_MODELS:\n",
    "    training.nn_training(\n",
    "        model = transformer_model,\n",
    "        train_loader = lyrics_train,\n",
    "        val_loader = lyrics_val,\n",
    "        embed_strategy = EMBED_STRATEGY,\n",
    "        learning_rate = LEARNING_RATE,\n",
    "        num_epochs = NUM_EPOCHS,\n",
    "        patience = PATIENCE,\n",
    "        verbose = True,\n",
    "        print_every = 1\n",
    "    )\n",
    "else:\n",
    "    state_dict = torch.load(f'models/{EMBED_STRATEGY}_DeepTransformer_Trained.pth', map_location = torch.device(DEVICE))\n",
    "    transformer_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy = 57.78%\n",
      "Training Improvement On Accuracy = +44.44%\n"
     ]
    }
   ],
   "source": [
    "# evaluate model performance - post traing\n",
    "if TRAIN_MODELS:\n",
    "    post_train_acc = training.evaluate_nn_model_against_test_set(\n",
    "        model = transformer_model,\n",
    "        test_dataset = lyrics_test\n",
    "    )\n",
    "\n",
    "    print(f'Training Improvement On Accuracy = +{(post_train_acc - pre_train_acc) * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tune GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 300/300 [00:01<00:00, 239.60 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Length = 210\n",
      "Val:   Length = 45\n",
      "Test:  Length = 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# create GPT2 Fine Tuning Datasets\n",
    "lyrics_gpt_train, lyrics_gpt_val, lyrics_gpt_test, gpt_tokenizer = preprocessing.gpt2_create_datasets(\n",
    "    data = lyrics, \n",
    "    label_mapping = genre_map,\n",
    "    input_col = 'lyrics', label_col = 'genre',\n",
    "    val_pct = VAL_PCT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2ForSequenceClassification(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D(nf=2304, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=768)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D(nf=3072, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=3072)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (score): Linear(in_features=768, out_features=6, bias=False)\n",
      ")\n",
      "GPT2 Fine Tuning: Start Time = 2025-03-30 13:50:35\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='159' max='159' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [159/159 27:57, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.827100</td>\n",
       "      <td>1.777803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.655400</td>\n",
       "      <td>1.588272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.298900</td>\n",
       "      <td>1.541120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2 Fine Tuning: Trained. Check the models subfolder for the trained model.\n",
      "GPT2 Fine Tuning: End Time = 2025-03-30 13:50:35, Duration = 28.22min\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2 Fine Tuning: Test Performance...\n",
      "\teval_loss: 1.537161111831665\n",
      "\teval_runtime: 30.4086\n",
      "\teval_samples_per_second: 1.48\n",
      "\teval_steps_per_second: 0.395\n",
      "\tepoch: 3.0\n"
     ]
    }
   ],
   "source": [
    "# fine tune the GPT2 model\n",
    "gpt2_model_fine_tuned = training.gpt2_fine_tuning(\n",
    "    train_dataset = lyrics_gpt_train,\n",
    "    val_dataset = lyrics_gpt_val,\n",
    "    test_dataset = lyrics_gpt_test,\n",
    "    input_tokenizer = gpt_tokenizer,\n",
    "    num_labels = len(genre_map),\n",
    "    batch_size = 4, # BATCH_SIZE,\n",
    "    num_epochs = 3, # NUM_EPOCHS,\n",
    "    learning_rate = LEARNING_RATE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Lyric Genre Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test lyrics\n",
    "pop_test = \"\"\"\"\n",
    "One. Don't pick up the phone. You know he's only calling cause he's drunk and alone.\n",
    "Two. Don't let him in. You'll have to kick him out again.\n",
    "Three. Don't be a friend. Cause you know you'll only wake up in his bed in the morning.\n",
    "Cause if you're under him. You're not getting over him.\n",
    "\"\"\"\n",
    "\n",
    "rock_test = \"\"\"\n",
    "We come from the land of the ice and snow\n",
    "From the midnight sun where the hot springs blow\n",
    "The hammer of the gods will drive our ships to new lands\n",
    "To fight the horde, singing and crying: Valhalla, I am coming!\n",
    "On we sweep with threshing oar\n",
    "Our only goal will be the western shore\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lyrics:\n",
      "\n",
      "We come from the land of the ice and snow\n",
      "From the midnight sun where the hot springs blow\n",
      "The hammer of the gods will drive our ships to new lands\n",
      "To fight the horde, singing and crying: Valhalla, I am coming!\n",
      "On we sweep with threshing oar\n",
      "Our only goal will be the western shore\n",
      "\n",
      "\n",
      "Predicted Genre: misc (idx = 1)\n"
     ]
    }
   ],
   "source": [
    "# make a prediction for a custom song\n",
    "if EMBED_STRATEGY == 'DistilBERT':\n",
    "    genre_classification.distilerbert_clf_prediction(\n",
    "        lyrics = rock_test,\n",
    "        clf_model = transformer_model, # base_model, transformer_model\n",
    "        label_mapping = genre_map,\n",
    "        device = DEVICE\n",
    "    )\n",
    "elif EMBED_STRATEGY == 'GloVe':\n",
    "    genre_classification.glove_clf_prediction(\n",
    "        lyrics = rock_test, # pop_test, rock_test\n",
    "        clf_model = transformer_model,\n",
    "        glove_index = glove_index,\n",
    "        label_mapping = genre_map,\n",
    "        max_seq_len = MAX_GLOVE_LENGTH,\n",
    "        device = DEVICE\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script End Time = 2025-03-30 14:19:20\n",
      "Duration = 32.90min\n"
     ]
    }
   ],
   "source": [
    "# wrap up\n",
    "END_TIME = datetime.now()\n",
    "SCRIPT_TIME = (END_TIME - START_TIME).seconds\n",
    "print(f'Script End Time = {END_TIME.strftime(\"%Y-%m-%d %H:%M:%S\")}')\n",
    "print(f'Duration = {SCRIPT_TIME / 60:.2f}min')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse6242-team157-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
