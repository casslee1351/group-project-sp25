{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE-6242 - Team 157 - Group Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TODO:__  \n",
    "1. Remove custom stop words like \"CHORUS:\" or \"INTRO:\"  \n",
    "2. Remove other stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global assumption panel\n",
    "\n",
    "## data gathering\n",
    "N_DATA_ROWS = 1500 # Use -1 to retrieve all rows\n",
    "\n",
    "## embedding\n",
    "EMBED_STRATEGY = 'DistilBERT' # ['DistilBERT', 'GloVe']\n",
    "\n",
    "## modeling - preprocessing\n",
    "VAL_PCT = 0.15 # the percent of data we want to withhold for testing\n",
    "BATCH_SIZE = 32 # bigger means faster training, but more memory use\n",
    "\n",
    "## modeling - architecture\n",
    "HIDDEN_SIZE = 256\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.2\n",
    "\n",
    "## modeling - training\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 50\n",
    "PATIENCE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages\n",
    "\n",
    "## torch\n",
    "import torch\n",
    "\n",
    "## project code\n",
    "from project_code import data_gathering\n",
    "from embedding import distilbert, glove\n",
    "from training import preprocessing\n",
    "from architectures import simple_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more global assumptions\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device = {DEVICE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data and perform basic cleaning operations\n",
    "lyrics = data_gathering.read_lyrics(n_rows = N_DATA_ROWS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # generate embeddings using word2vec\n",
    "# # if EMBED_STRATEGY == 'Word2Vec':\n",
    "# #     embedding_model = word2vec.apply_word2vec(lyrics['tokenized_text'].tolist())\n",
    "# if EMBED_STRATEGY == 'GloVe':\n",
    "#     lyrics_embed = glove.create_glove_matrix(data = lyrics, target_col = 'cleaned_lyrics')\n",
    "# elif EMBED_STRATEGY == 'DistilBERT':\n",
    "#     lyrics_embed = distilbert.distilbert_embed_all_docs(data = lyrics, target_col = 'lyrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_embed = distilbert.embed_all_docs_v2(data = lyrics, target_col = 'lyrics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter(lyrics.tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data loaders (train, val) and data sets (test)\n",
    "lyrics_train, lyrics_val, lyrics_test = preprocessing.create_datasets(\n",
    "    data_embed = lyrics_embed,\n",
    "    labels = lyrics['tag'],\n",
    "    val_pct = VAL_PCT,\n",
    "    batch_size = BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the hierarchical attention model\n",
    "n_songs, embed_dim = lyrics_embed.shape\n",
    "base_model = simple_rnn.SimpleRNN(\n",
    "    input_dim = embed_dim,\n",
    "    hidden_dim = HIDDEN_SIZE,\n",
    "    output_dim = len(lyrics['tag'].unique()),\n",
    "    type = 'GRU',\n",
    "    num_layers = NUM_LAYERS,\n",
    "    dropout = DROPOUT\n",
    ").to(DEVICE)\n",
    "base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "def nn_training(\n",
    "    model,\n",
    "    train_loader, val_loader,\n",
    "    learning_rate:float = 0.001,\n",
    "    num_epochs:int = 100, patience:int = 5,\n",
    "    device:str = 'cpu',\n",
    "    verbose:bool = True, print_every:int = 5\n",
    "):\n",
    "    \"\"\"\n",
    "    Description\n",
    "    ----------\n",
    "    This function takes a specified model and trains it on the train_loader,\n",
    "    evaluates the epoch round on the val_loader, and allows for early stopping\n",
    "    if there is no improvement against the val set after `patience` epochs.\n",
    "\n",
    "    Inputs\n",
    "    ----------\n",
    "    model = An instantiated torch neural network model that we want to train\n",
    "    train_loader = The DataLoader object containing our training set\n",
    "    val_loader = The DataLoader object containing our validation set\n",
    "    learning_rate = The rate at which we want to learn\n",
    "    num_epochs = The (max) number of epochs we want to train for\n",
    "    patience = The number of epochs beyond the current best epoch that we\n",
    "        keep training before stopping early\n",
    "    device = The device we are training on\n",
    "    verbose = If true, prints useful intermediates\n",
    "    print_every = If verbose, this defines how frequently we print out\n",
    "        model performance results\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    None, but the best model weights and biases will be stored in the models folder\n",
    "    \"\"\"\n",
    "    # set up training objects\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "    # instantiate constants\n",
    "    best_val_loss = float('inf')\n",
    "    best_epoch = 0\n",
    "    waiting = 0\n",
    "\n",
    "    # train the model\n",
    "    for epoch in range(num_epochs):       \n",
    "\n",
    "        # === training loop ===\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for X, y in tqdm(train_loader):\n",
    "\n",
    "            ## store batch to device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            ## forward pass\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X)\n",
    "\n",
    "            ## backward pass\n",
    "            loss = criterion(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            ## store loss from batch\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # === validation loop ===\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X, y in val_loader:\n",
    "\n",
    "                ## store batch to device\n",
    "                X, y = X.to(device), y.to(device)\n",
    "\n",
    "                ## forward pass\n",
    "                y_pred = model(X)\n",
    "\n",
    "                ## store loss from batch\n",
    "                loss = criterion(y_pred, y)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        # epoch wrap up\n",
    "\n",
    "        ## take average of loss\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        ## check for early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_epoch = epoch\n",
    "            waiting = 0\n",
    "            torch.save(model.state_dict(), f'models/MGC {model.__class__.__name__}.pth')\n",
    "        else:\n",
    "            waiting += 1\n",
    "\n",
    "\n",
    "        if verbose:\n",
    "            if (epoch + 1) % print_every == 0:\n",
    "                if val_loss < best_val_loss:\n",
    "                    print(\n",
    "                        f'[{epoch + 1} / {num_epochs}] '\n",
    "                        f'Train Loss = {train_loss:.4f}, '\n",
    "                        f'Val Loss = {val_loss:.4f} '\n",
    "                        f'**New Best Model**'\n",
    "                    )\n",
    "                else:\n",
    "                    print(\n",
    "                        f'[{epoch + 1} / {num_epochs}] '\n",
    "                        f'Train Loss = {train_loss:.4f}, '\n",
    "                        f'Val Loss = {val_loss:.4f}'\n",
    "                    )\n",
    "\n",
    "        if waiting >= patience:\n",
    "            print(f'Early Stopping Triggered. Training Stopped.')\n",
    "            print(f'\\tBest Epoch = {best_epoch}, Best Val Loss = {best_val_loss}')\n",
    "            break\n",
    "\n",
    "nn_training(\n",
    "    model = base_model,\n",
    "    train_loader = lyrics_train,\n",
    "    val_loader = lyrics_val,\n",
    "    learning_rate = LEARNING_RATE,\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    patience = PATIENCE,\n",
    "    verbose = True,\n",
    "    print_every = 1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse-6242-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
